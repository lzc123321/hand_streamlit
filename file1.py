import streamlit as st
import cv2
import numpy as np

import torch
import tempfile
from PIL import Image
import torch
from yolov5.models.common import DetectMultiBackend
from pydub import AudioSegment
import pathlib

temp = pathlib.PosixPath
pathlib.PosixPath = pathlib.WindowsPath

# è·å–éŸ³é¢‘æ–‡ä»¶çš„é•¿åº¦
def get_audio_length(file_path):
    audio = AudioSegment.from_wav(file_path)
    return audio.duration_seconds  # è¿”å›éŸ³é¢‘çš„é•¿åº¦ï¼ˆç§’ï¼‰

@st.cache_resource
def load_model():
    # Model loading
    model = torch.hub.load("ultralytics/yolov5",
                           "custom", path="best_m.pt", force_reload=True,
                           autoshape=True)  # Can be 'yolov5n' - 'yolov5x6', or 'custom'
    return model


demo_img = "fire.png"
demo_video = "å¥½è¿æ¥.mp4"

st.title('Gesture Recognition')
st.logo("logo_w.png", size="large")
sidebar_markdown_title = '''
    <center>
        <h1>Gesture recognition</h1>
    </center>
    '''
sidebar_markdown = '''
    <div style="text-align: center;">
        <code> v0.2.1 </code>
    </div>
    
    <center>
    <a href="https://github.com/lzc123321/hand_streamlit">
    <img src = "https://cdn-icons-png.flaticon.com/512/733/733609.png" width="23"></img></a>

    <a href="mailto:kathrin.sessler@tum.de">
    <img src="https://cdn-icons-png.flaticon.com/512/646/646094.png" alt="email" width = "27" ></a>
    </center>
    <br> <!-- æ·»åŠ ç©ºè¡Œ -->

    '''.format()
st.sidebar.image("3.png", use_container_width=True)
st.sidebar.markdown(sidebar_markdown_title, unsafe_allow_html=True)

st.sidebar.markdown(sidebar_markdown, unsafe_allow_html=True)
st.sidebar.image("2.png", use_container_width=True)

app_mode = st.sidebar.selectbox('Choose the App Mode',
                                ['About App', 'About models', 'Run on Image', 'Run on Video', 'Run on WebCam'])

if app_mode == 'About App':
    st.markdown("<h5>This is the Gesture Recognition App created with custom trained models using YoloV5</h5>",
                unsafe_allow_html=True)

    # st.markdown("- <h5>Select the App Mode in the SideBar</h5>",unsafe_allow_html=True)
    # st.markdown("- <h6>Upload the Image and Detect the Gesture in Images</h5>",unsafe_allow_html=True)
    # #st.image("mode2.png")
    # st.markdown("- <h6>Upload the Video and Detect the Gestures in Videos</h5>",unsafe_allow_html=True)
    # #st.image("Images/third_3.png")
    # st.markdown("- <h6>Live Detection</h5>",unsafe_allow_html=True)
    #st.image("Images/fourth_4.png")
    # st.markdown("- <h5>Click Start to start the camera</h5>",unsafe_allow_html=True)
    # st.markdown("- <h5>Click Stop to stop the camera</h5>",unsafe_allow_html=True)

    st.markdown("""
## â˜‘ï¸Select the App Mode in the SideBarâ˜‘ï¸
- Upload the Image and Detect the Gesture in Images Detection
- Upload the Video and Detect the Gestures in Videos Detection
- Live
## â­Featuresâ­
- Detect on Image
- Detect on Videos
- Live Detection
## ğŸ§©Tech StackğŸ§©
- Python
- PyTorch
- Python CV
- Streamlit
- YoloV5
## ğŸ“§ Contact  ğŸ“§ è”ç³»

""")
if app_mode == 'About models':
    st.markdown(
        "<h5>YoloV5 is a family of object detection architectures and models pretrained on the COCO dataset.</h5>",
        unsafe_allow_html=True)
    st.image("1.jpg", use_container_width=True)
    st.markdown("""

    ## Citations
    - HaGRIDv2: Nuzhdin A., Nagaev A., et al. "HaGRIDv2: 1M Images for Static and Dynamic Hand Gesture Recognition", arXiv:2412.01508 (2024). 
      Available: https://arxiv.org/abs/2412.01508

    - HaGRID: Kapitanov A., Kvanchiani K., et al. "HaGRID - HAnd Gesture Recognition Image Dataset", WACV 2024, pp. 4572-4581. 
      Available: https://openaccess.thecvf.com/content/WACV2024/html/Kapitanov_HaGRID_-_HAnd_Gesture_Recognition_Image_Dataset_WACV_2024_paper.html

    """)

if app_mode == 'Run on Image':
    st.subheader("Results")
    text = st.markdown("")

    st.sidebar.markdown("---")
    # Input for Image
    img_file = st.sidebar.file_uploader("Upload an Image", type=["jpg", "jpeg", "png"])
    if img_file:
        image = np.array(Image.open(img_file))
    else:
        image = np.array(Image.open(demo_img))

    st.sidebar.markdown("---")
    st.sidebar.markdown("**Original Image**")
    st.sidebar.image(image)

    # predict the image
    model = load_model()
    results = model(image, augment=True)
    print(results.xyxy[0])
    length = len(results.xyxy[0])
    output = np.squeeze(results.render())
    text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>", unsafe_allow_html=True)
    st.subheader("Output Image")
    st.image(output, use_container_width=True)

# if app_mode == 'Run on Video':
#     st.subheader("Detected Fire:")
#     text = st.markdown("")
#
#     st.sidebar.markdown("---")
#
#     st.subheader("Output")
#     stframe = st.empty()
#
#     #Input for Video
#     video_file = st.sidebar.file_uploader("Upload a Video",type=['mp4','mov','avi','asf','m4v'])
#     st.sidebar.markdown("---")
#     tffile = tempfile.NamedTemporaryFile(delete=False)
#
#     if not video_file:
#         vid = cv2.VideoCapture(demo_video)
#         tffile.name = demo_video
#     else:
#         tffile.write(video_file.read())
#         vid = cv2.VideoCapture(tffile.name)
#
#     st.sidebar.markdown("**Input Video**")
#     st.sidebar.video(tffile.name)
#
#     # predict the video
#     while vid.isOpened():
#         ret, frame = vid.read()
#         if not ret:
#             break
#         frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
#         model = load_model()
#         results = model(frame)
#         length = len(results.xyxy[0])
#         output = np.squeeze(results.render())
#         text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>",unsafe_allow_html = True)
#         stframe.image(output)
import cv2
import tempfile
import os
from datetime import datetime

if app_mode == 'Run on Video':
    st.subheader("Detected Gesture:")
    text = st.markdown("")
    st.sidebar.markdown("---")
    st.subheader("Output")
    stframe = st.empty()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "output_videos"
    os.makedirs(output_dir, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = os.path.join(output_dir, f"new_video_{timestamp}.mp4")

    # è§†é¢‘è¾“å…¥å¤„ç†
    video_file = st.sidebar.file_uploader("Upload a Video", type=['mp4', 'mov', 'avi', 'asf', 'm4v'])
    st.sidebar.markdown("---")
    tffile = tempfile.NamedTemporaryFile(delete=False)
    # æ·»åŠ å¼€å§‹æŒ‰é’®
    start_button = st.sidebar.button("å¼€å§‹")

    if video_file:
        if start_button:
            tffile.write(video_file.read())
            tffile.flush()
            vid = cv2.VideoCapture(tffile.name)

            # è·å–åŸè§†é¢‘å‚æ•°
            width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = vid.get(cv2.CAP_PROP_FPS)

            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼ˆä½¿ç”¨H.264ç¼–ç ï¼‰
            fourcc = cv2.VideoWriter_fourcc(*'H264')  # æˆ–ä½¿ç”¨ 'avc1' éœ€è¦å¯¹åº”ç¯å¢ƒæ”¯æŒ
            out = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))

            # æå‰åŠ è½½æ¨¡å‹ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
            model = load_model()  # å°†æ¨¡å‹åŠ è½½ç§»å‡ºå¾ªç¯

            # æ˜¾ç¤ºåŸè§†é¢‘
            st.sidebar.markdown("**è¾“å…¥è§†é¢‘**")
            st.sidebar.video(tffile.name)

            # è¿›åº¦æ¡
            progress_bar = st.progress(0)
            total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))

            # å¤„ç†è§†é¢‘å¸§
            frame_count = 0
            while vid.isOpened():
                ret, frame = vid.read()
                if not ret:
                    break

                # å¤„ç†å¸§
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = model(frame)
                output = np.squeeze(results.render())

                # å†™å…¥å¤„ç†åçš„å¸§åˆ°è§†é¢‘æ–‡ä»¶ï¼ˆéœ€è¦è½¬æ¢å›BGRï¼‰
                out.write(cv2.cvtColor(output, cv2.COLOR_RGB2BGR))

                # æ›´æ–°ç•Œé¢æ˜¾ç¤º
                length = len(results.xyxy[0])
                text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>", unsafe_allow_html=True)
                stframe.image(output)

                # æ›´æ–°è¿›åº¦æ¡
                frame_count += 1
                progress = min(frame_count / total_frames, 1.0)
                progress_bar.progress(progress)
            # é‡Šæ”¾èµ„æº
            vid.release()
            out.release()
            cv2.destroyAllWindows()

            # æ˜¾ç¤ºå®Œæˆæç¤ºå’Œä¸‹è½½æŒ‰é’®
            st.success(f"è§†é¢‘å¤„ç†å®Œæˆï¼ä¿å­˜è·¯å¾„: {output_filename}")
            # æ–°å¢æ’­æ”¾æ§åˆ¶ç»„ä»¶ ----------------------------------------
            st.markdown("---")
            # æ˜¾ç¤ºåŸè§†é¢‘

            video_file = open(output_filename, 'rb')
            video_bytes = video_file.read()

            # æœ¬åœ°è§†é¢‘
            st.video(video_bytes, format="mp4", start_time=0)

            # è¿›åº¦æ¡
            progress_bar = st.progress(0)
            total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))

            with open(output_filename, "rb") as file:
                st.download_button(
                    label="ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
                    data=file,
                    file_name=os.path.basename(output_filename),
                    mime="video/mp4"
                )

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tffile.name)
        else:
            st.sidebar.markdown("è¯·æŒ‰ä¸‹å¼€å§‹æŒ‰é’®ä»¥åŠ è½½æ¨¡å‹å¹¶å¤„ç†è§†é¢‘ã€‚")
    else:
        st.sidebar.markdown("è¯·ä¸Šä¼ ä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„è§†é¢‘æ–‡ä»¶ã€‚")
    # if video_file:
    #     vid = cv2.VideoCapture(demo_video)
    #     tffile.name = demo_video
    # else:
    #     tffile.write(video_file.read())
    #     vid = cv2.VideoCapture(tffile.name)
    #
    # # è·å–åŸè§†é¢‘å‚æ•°
    # width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))
    # height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
    # fps = vid.get(cv2.CAP_PROP_FPS)
    #
    # # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼ˆä½¿ç”¨H.264ç¼–ç ï¼‰
    # fourcc = cv2.VideoWriter_fourcc(*'H264')  # æˆ–ä½¿ç”¨ 'avc1' éœ€è¦å¯¹åº”ç¯å¢ƒæ”¯æŒ
    # out = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))
    #
    # # æå‰åŠ è½½æ¨¡å‹ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    # model = load_model()  # å°†æ¨¡å‹åŠ è½½ç§»å‡ºå¾ªç¯
    #
    # # æ˜¾ç¤ºåŸè§†é¢‘
    # st.sidebar.markdown("**Input Video**")
    # st.sidebar.video(tffile.name)

    # # è¿›åº¦æ¡
    # progress_bar = st.progress(0)
    # total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))
    #
    # # å¤„ç†è§†é¢‘å¸§
    # frame_count = 0
    # while vid.isOpened():
    #     ret, frame = vid.read()
    #     if not ret:
    #         break
    #
    #     # å¤„ç†å¸§
    #     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    #     results = model(frame)
    #     output = np.squeeze(results.render())
    #
    #     # å†™å…¥å¤„ç†åçš„å¸§åˆ°è§†é¢‘æ–‡ä»¶ï¼ˆéœ€è¦è½¬æ¢å›BGRï¼‰
    #     out.write(cv2.cvtColor(output, cv2.COLOR_RGB2BGR))
    #
    #     # æ›´æ–°ç•Œé¢æ˜¾ç¤º
    #     length = len(results.xyxy[0])
    #     text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>", unsafe_allow_html=True)
    #     stframe.image(output)
    #
    #     # æ›´æ–°è¿›åº¦æ¡
    #     frame_count += 1
    #     progress = min(frame_count / total_frames, 1.0)
    #     progress_bar.progress(progress)

    # # é‡Šæ”¾èµ„æº
    # vid.release()
    # out.release()
    # cv2.destroyAllWindows()
    #
    # # æ˜¾ç¤ºå®Œæˆæç¤ºå’Œä¸‹è½½æŒ‰é’®
    # st.success(f"è§†é¢‘å¤„ç†å®Œæˆï¼ä¿å­˜è·¯å¾„: {output_filename}")

    # # æ–°å¢æ’­æ”¾æ§åˆ¶ç»„ä»¶ ----------------------------------------
    # st.markdown("---")
    # # æ˜¾ç¤ºåŸè§†é¢‘
    #
    # video_file = open(output_filename, 'rb')
    # video_bytes = video_file.read()
    #
    # # æœ¬åœ°è§†é¢‘
    # st.video(video_bytes, format="mp4", start_time=0)
    #
    # # è¿›åº¦æ¡
    # progress_bar = st.progress(0)
    # total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))
    #
    # with open(output_filename, "rb") as file:
    #     st.download_button(
    #         label="ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
    #         data=file,
    #         file_name=os.path.basename(output_filename),
    #         mime="video/mp4"
    #     )
    #
    # # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    # os.unlink(tffile.name)
str1 = ""
class_name = "None"
import time
import shutil

if app_mode == 'Run on WebCam':
    st.subheader("Detected Gesture:")
    text = st.markdown("")

    st.sidebar.markdown("---")

    st.subheader("Output")
    stframe = st.empty()

    run, stop = st.sidebar.columns(2)

    # åœ¨ç¬¬ä¸€åˆ—æ”¾ç½® "Start" æŒ‰é’®
    with run:
        run = st.button("ğŸ”›StartğŸ”›", use_container_width=True)

    # åœ¨ç¬¬äºŒåˆ—æ”¾ç½® "Stop" æŒ‰é’®
    with stop:
        stop = st.button("â¹Stopâ¹", use_container_width=True)
    st.sidebar.markdown("---")

    cam = cv2.VideoCapture(0)
    # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸
    hand_type = {}
    # è®¡æ—¶å™¨
    time_start = 0
    # ç»“æœç±»åˆ«
    result_cls = ""
    class_name_ti10 = ""
    if (run):
        while (True):
            if (stop):
                break
            # åˆå§‹åŒ–è®¡æ—¶å™¨å’Œå¸§è®¡æ•°å™¨
            start_time = time.time()
            frame_count = 0
            ret, frame = cam.read()
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            model = load_model()
            results = model(frame)
            # æ›´æ–°å¸§è®¡æ•°å™¨
            frame_count += 1

            # è®¡ç®—å¹¶æ˜¾ç¤ºFPS
            elapsed_time = time.time() - start_time
            print(f"FPS: {frame_count / elapsed_time:.2f}")
            length = 0
            # html_content = f"""
            #         <div style='text-align: center;'>
            #             <h1 style='color: red;'>fps: {frame_count / elapsed_time:.2f}</h1>
            #         </div>
            # """
            # text.write(html_content, unsafe_allow_html=True)
            # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æµ‹åˆ°ä»»ä½•æ¡†
            if len(results.xyxy[0]) > 0:
                # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„æ¡†
                highest_confidence = -1
                highest_confidence_box = None
                highest_cls = None

                for *box, conf, cls in results.xyxy[0]:
                    if conf > highest_confidence:
                        highest_confidence = conf
                        highest_confidence_box = box
                        highest_cls = cls

                # ç¡®ä¿ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼
                if highest_confidence > 0.8:
                    time_start += 1
                    # å°†æœ€é«˜ç½®ä¿¡åº¦çš„æ¡†è½¬æ¢ä¸ºæ•´æ•°åæ ‡
                    x1, y1, x2, y2 = map(int, highest_confidence_box)
                    class_id = int(highest_cls)
                    class_name = results.names[class_id]
                    confidence = float(highest_confidence)

                    # æ‰“å°ç±»åˆ«åç§°
                    print(f"æ£€æµ‹åˆ°çš„ç±»åˆ«: {class_name}")

                    # åœ¨åŸå§‹å›¾åƒä¸Šç»˜åˆ¶æœ€é«˜ç½®ä¿¡åº¦çš„æ¡†
                    frame_with_box = frame.copy()
                    cv2.rectangle(frame_with_box, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame_with_box, f"{class_name} {confidence:.2f}", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

                    # æ˜¾ç¤ºå›¾åƒ
                    stframe.image(frame_with_box)
                    if time_start > 10:
                        time_start = 0
                        class_name_ti10 = class_name
                        audio_container = st.container()  # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥æ˜¾ç¤ºéŸ³é¢‘
                        audio_path = f"voices_hands/{class_name_ti10}.wav"
                        # # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶å
                        # timestamp = datetime.now().strftime("%Y%m%d%H%M%S%f")
                        # temp_audio_path = f"voices_hands/temp_{timestamp}.wav"

                        # # å¤åˆ¶åŸå§‹éŸ³é¢‘æ–‡ä»¶
                        # shutil.copyfile(audio_path, temp_audio_path)
                        audio_bytes = open(audio_path, 'rb').read()
                        with audio_container:
                            # è·å–éŸ³é¢‘æ–‡ä»¶çš„é•¿åº¦
                            #audio_length = get_audio_length(audio_bytes)
                            st.audio(audio_bytes,format="audio/wav", autoplay =True)
                        print(f"æ’­æ”¾éŸ³é¢‘æ—¶é—´: {audio_bytes}")
                        time.sleep(5)  # å‡è®¾éŸ³é¢‘é•¿åº¦ä¸º2ç§’ï¼Œç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæ¯•
                        audio_container.empty()  # æ¸…ç©ºå®¹å™¨ä»¥ç§»é™¤éŸ³é¢‘ç»„ä»¶
                        # if os.path.exists(temp_audio_path):
                        #     os.remove(temp_audio_path)
                else:
                    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦çš„æ¡†ï¼Œæ˜¾ç¤ºåŸå§‹å›¾åƒ
                    stframe.image(frame)
            else:
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•æ¡†ï¼Œæ˜¾ç¤ºåŸå§‹å›¾åƒ
                stframe.image(frame)
            html_content = f"""
                                    <div style='text-align: center;'>
                                         <h1 style='color: red;'>fps: {frame_count / elapsed_time:.2f}</h1>
                                        <h1 style='color: red;'>{class_name_ti10}</h1>
                                    </div>
                                    """
            text.write(html_content, unsafe_allow_html=True)
            # # éå†æ£€æµ‹ç»“æœ
            # for det in results.xyxy[0]:
            #     # æå–ç±»åˆ« ID
            #     class_id = int(det[5])
            #     confidence = float(det[4])
            #     # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ£€æµ‹ç»“æœ
            #     # è·å–ç±»åˆ«åç§°
            #     class_name = results.names[class_id]
            #
            #     # æ‰“å°ç±»åˆ«åç§°
            #     print(f"æ£€æµ‹åˆ°çš„ç±»åˆ«: {class_name}")
            #     if confidence > 0.7:
            #         length = len(results.xyxy[0])
            #         hand_type[class_name] = hand_type.get(class_name, 0) + 1
            #         time_start += 1
            #         output = np.squeeze(results.render())
            #         stframe.image(output)
            #     else:
            #         stframe.image(frame)
            #
            #     if time_start > 10:
            #         # è·å–å­—å…¸ä¸­maxé”®å€¼å¯¹
            #         max_key = max(hand_type, key=hand_type.get)
            #         result_cls = max_key
            #         # æ‰“å°æœ€å¤§é”®å€¼å¯¹
            #         print(f"æœ€å¤§ç±»åˆ«: {max_key}, æ•°é‡: {hand_type[max_key]}")
            #
            #         time_start = 0
            #         hand_type = {}
            #         break
            #
            #     html_content = f"""
            #     <div style='text-align: center;'>
            #         <h1 style='color: red;'>{length}</h1>
            #         <h1 style='color: red;'>{result_cls}</h1>
            #     </div>
            #     """
            #     text.write(html_content, unsafe_allow_html=True)
            # stframe.image(frame)

