import streamlit as st
import cv2
import numpy as np

import torch
import tempfile
from PIL import Image
import torch
from yolov5.models.common import DetectMultiBackend

import pathlib
temp = pathlib.PosixPath
pathlib.PosixPath = pathlib.WindowsPath



@st.cache_resource
def load_model():
    # Model loading
    model = torch.hub.load("ultralytics/yolov5", "custom", path="yolov5s.pt", force_reload=True) # Can be 'yolov5n' - 'yolov5x6', or 'custom'
    return model

demo_img = "fire.png"
demo_video = "å¥½è¿æ¥.mp4"

st.title('Fire Detection')
st.sidebar.title('App Mode')


app_mode = st.sidebar.selectbox('Choose the App Mode',
                                ['About App','Run on Image','Run on Video','Run on WebCam'])

if app_mode == 'About App':
    st.subheader("About")
    st.markdown("<h5>This is the Fire Detection App created with custom trained models using YoloV5</h5>",unsafe_allow_html=True)

    st.markdown("- <h5>Select the App Mode in the SideBar</h5>",unsafe_allow_html=True)
    #st.image("Images/first_1.png")
    st.markdown("- <h5>Upload the Image and Detect the Fires in Images</h5>",unsafe_allow_html=True)
    #st.image("Images/second_2.png")
    st.markdown("- <h5>Upload the Video and Detect the fires in Videos</h5>",unsafe_allow_html=True)
    #st.image("Images/third_3.png")
    st.markdown("- <h5>Live Detection</h5>",unsafe_allow_html=True)
    #st.image("Images/fourth_4.png")
    st.markdown("- <h5>Click Start to start the camera</h5>",unsafe_allow_html=True)
    st.markdown("- <h5>Click Stop to stop the camera</h5>",unsafe_allow_html=True)

    st.markdown("""
                ## Features
- Detect on Image
- Detect on Videos
- Live Detection
## Tech Stack
- Python
- PyTorch
- Python CV
- Streamlit
- YoloV5
## ğŸ”— Links
[![twitter](https://img.shields.io/badge/Github-1DA1F2?style=for-the-badge&logo=github&logoColor=white)](https://github.com/AntroSafin)
""")


if app_mode == 'Run on Image':
    st.subheader("Detected Fire:")
    text = st.markdown("")

    st.sidebar.markdown("---")
    # Input for Image
    img_file = st.sidebar.file_uploader("Upload an Image",type=["jpg","jpeg","png"])
    if img_file:
        image = np.array(Image.open(img_file))
    else:
        image = np.array(Image.open(demo_img))

    st.sidebar.markdown("---")
    st.sidebar.markdown("**Original Image**")
    st.sidebar.image(image)

    # predict the image
    model = load_model()
    results = model(image)
    length = len(results.xyxy[0])
    output = np.squeeze(results.render())
    text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>",unsafe_allow_html = True)
    st.subheader("Output Image")
    st.image(output,use_column_width=True)

# if app_mode == 'Run on Video':
#     st.subheader("Detected Fire:")
#     text = st.markdown("")
#
#     st.sidebar.markdown("---")
#
#     st.subheader("Output")
#     stframe = st.empty()
#
#     #Input for Video
#     video_file = st.sidebar.file_uploader("Upload a Video",type=['mp4','mov','avi','asf','m4v'])
#     st.sidebar.markdown("---")
#     tffile = tempfile.NamedTemporaryFile(delete=False)
#
#     if not video_file:
#         vid = cv2.VideoCapture(demo_video)
#         tffile.name = demo_video
#     else:
#         tffile.write(video_file.read())
#         vid = cv2.VideoCapture(tffile.name)
#
#     st.sidebar.markdown("**Input Video**")
#     st.sidebar.video(tffile.name)
#
#     # predict the video
#     while vid.isOpened():
#         ret, frame = vid.read()
#         if not ret:
#             break
#         frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
#         model = load_model()
#         results = model(frame)
#         length = len(results.xyxy[0])
#         output = np.squeeze(results.render())
#         text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>",unsafe_allow_html = True)
#         stframe.image(output)
import cv2
import tempfile
import os
from datetime import datetime

if app_mode == 'Run on Video':
    st.subheader("Detected Fire:")
    text = st.markdown("")
    st.sidebar.markdown("---")
    st.subheader("Output")
    stframe = st.empty()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "output_videos"
    os.makedirs(output_dir, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = os.path.join(output_dir, f"new_video_{timestamp}.mp4")

    # è§†é¢‘è¾“å…¥å¤„ç†
    video_file = st.sidebar.file_uploader("Upload a Video", type=['mp4', 'mov', 'avi', 'asf', 'm4v'])
    st.sidebar.markdown("---")
    tffile = tempfile.NamedTemporaryFile(delete=False)

    if not video_file:
        vid = cv2.VideoCapture(demo_video)
        tffile.name = demo_video
    else:
        tffile.write(video_file.read())
        vid = cv2.VideoCapture(tffile.name)

    # è·å–åŸè§†é¢‘å‚æ•°
    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = vid.get(cv2.CAP_PROP_FPS)

    # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼ˆä½¿ç”¨H.264ç¼–ç ï¼‰
    fourcc = cv2.VideoWriter_fourcc(*'H264')  # æˆ–ä½¿ç”¨ 'avc1' éœ€è¦å¯¹åº”ç¯å¢ƒæ”¯æŒ
    out = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))

    # æå‰åŠ è½½æ¨¡å‹ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    model = load_model()  # å°†æ¨¡å‹åŠ è½½ç§»å‡ºå¾ªç¯

    # æ˜¾ç¤ºåŸè§†é¢‘
    st.sidebar.markdown("**Input Video**")
    st.sidebar.video(tffile.name)

    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))

    # å¤„ç†è§†é¢‘å¸§
    frame_count = 0
    while vid.isOpened():
        ret, frame = vid.read()
        if not ret:
            break

        # å¤„ç†å¸§
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = model(frame)
        output = np.squeeze(results.render())

        # å†™å…¥å¤„ç†åçš„å¸§åˆ°è§†é¢‘æ–‡ä»¶ï¼ˆéœ€è¦è½¬æ¢å›BGRï¼‰
        out.write(cv2.cvtColor(output, cv2.COLOR_RGB2BGR))

        # æ›´æ–°ç•Œé¢æ˜¾ç¤º
        length = len(results.xyxy[0])
        text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>", unsafe_allow_html=True)
        stframe.image(output)

        # æ›´æ–°è¿›åº¦æ¡
        frame_count += 1
        progress = min(frame_count / total_frames, 1.0)
        progress_bar.progress(progress)

    # é‡Šæ”¾èµ„æº
    vid.release()
    out.release()
    cv2.destroyAllWindows()

    # æ˜¾ç¤ºå®Œæˆæç¤ºå’Œä¸‹è½½æŒ‰é’®
    st.success(f"è§†é¢‘å¤„ç†å®Œæˆï¼ä¿å­˜è·¯å¾„: {output_filename}")

    # æ–°å¢æ’­æ”¾æ§åˆ¶ç»„ä»¶ ----------------------------------------
    st.markdown("---")
    # æ˜¾ç¤ºåŸè§†é¢‘

    video_file = open(output_filename, 'rb')
    video_bytes = video_file.read()

    # æœ¬åœ°è§†é¢‘
    st.video(video_bytes, format="mp4", start_time=0)

    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))


    with open(output_filename, "rb") as file:
        st.download_button(
            label="ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
            data=file,
            file_name=os.path.basename(output_filename),
            mime="video/mp4"
        )

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.unlink(tffile.name)
str = ""
if app_mode == 'Run on WebCam':
    st.subheader("Detected Fire:")
    text = st.markdown("")

    st.sidebar.markdown("---")

    st.subheader("Output")
    stframe = st.empty()

    run = st.sidebar.button("Start")
    stop = st.sidebar.button("Stop")
    st.sidebar.markdown("---")

    cam = cv2.VideoCapture(0)
    # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸
    hand_type = {}
    # è®¡æ—¶å™¨
    time_start = 0
    # ç»“æœç±»åˆ«
    result_cls = ""
    if(run):
        while(True):
            if(stop):
                break
            ret,frame = cam.read()
            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)
            model = load_model()
            results = model(frame)
            length = len(results.xyxy[0])

            # éå†æ£€æµ‹ç»“æœ
            for det in results.xyxy[0]:
                # æå–ç±»åˆ« ID
                class_id = int(det[5])
                confidence = float(det[4])
                # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ£€æµ‹ç»“æœ
                # è·å–ç±»åˆ«åç§°
                class_name = results.names[class_id]

                # æ‰“å°ç±»åˆ«åç§°
                print(f"æ£€æµ‹åˆ°çš„ç±»åˆ«: {class_name}")
                if confidence > 0.75:
                    hand_type[class_name] = hand_type.get(class_name, 0) + 1
                    time_start += 1
                    output = np.squeeze(results.render())
                    stframe.image(output)
                else:
                    stframe.image(frame)

                if time_start > 10:
                    # è·å–å­—å…¸ä¸­maxé”®å€¼å¯¹
                    max_key = max(hand_type, key=hand_type.get)
                    result_cls = max_key
                    # æ‰“å°æœ€å¤§é”®å€¼å¯¹
                    print(f"æœ€å¤§ç±»åˆ«: {max_key}, æ•°é‡: {hand_type[max_key]}")

                    time_start = 0
                    hand_type = {}
                text.write(f"<h1 style='text-align: center; color:red;'>{length}</h1>",unsafe_allow_html = True)
                text.write(f"<h1 style='text-align: center; color:red;'>{result_cls}</h1>", unsafe_allow_html=True)
            stframe.image(frame)
